{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dataset for Basenji training involves a series of design choices.\n",
    "\n",
    "The input you bring to the pipeline is:\n",
    "* BigWig coverage tracks\n",
    "* Genome FASTA file\n",
    "\n",
    "First, make sure you have an hg19 FASTA file visible. If you have it already, put a symbolic link into the data directory. Otherwise, I have a machine learning friendly simplified version you can download in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "if not os.path.isfile('data/hg19.ml.fa'):\n",
    "    subprocess.call('curl -o data/hg19.ml.fa https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa', shell=True)\n",
    "    subprocess.call('curl -o data/hg19.ml.fa.fai https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa.fai', shell=True)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab a few CAGE datasets from FANTOM5 related to heart biology.\n",
    "\n",
    "These data were processed by\n",
    "1. Aligning with Bowtie2 with very sensitive alignment parameters.\n",
    "2. Distributing multi-mapping reads and estimating genomic coverage with [bam_cov.py](https://github.com/calico/basenji/blob/master/bin/bam_cov.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/CNhs11760.bw'):\n",
    "    subprocess.call('curl -o data/CNhs11760.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs11760.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12843.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12843.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12856.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12856.bw', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll write out these BigWig files and labels to a samples table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [['index','identifier','file','clip','sum_stat','description']]\n",
    "lines.append(['0', 'CNhs11760', 'data/CNhs11760.bw', '384', 'sum', 'aorta'])\n",
    "lines.append(['1', 'CNhs12843', 'data/CNhs12843.bw', '384', 'sum', 'artery'])\n",
    "lines.append(['2', 'CNhs12856', 'data/CNhs12856.bw', '384', 'sum', 'pulmonic_valve'])\n",
    "\n",
    "samples_out = open('data/heart_wigs.txt', 'w')\n",
    "for line in lines:\n",
    "    print('\\t'.join(line), file=samples_out)\n",
    "samples_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to choose genomic sequences to form batches for stochastic gradient descent, divide them into training/validation/test sets, and construct TFRecords to provide to downstream programs.\n",
    "\n",
    "The script [basenji_data.py](https://github.com/calico/basenji/blob/master/bin/basenji_data.py) implements this procedure.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| -d | 0.1 | Down-sample the genome to 10% to speed things up here. |\n",
    "| -g | data/unmap_macro.bed | Dodge large-scale unmappable regions like assembly gaps. |\n",
    "| -l | 131072 | Sequence length. |\n",
    "| --local | True | Run locally, as opposed to on my SLURM scheduler. |\n",
    "| -o | data/heart_l131k | Output directory |\n",
    "| -p | 8 | Uses multiple concourrent processes to read/write. |\n",
    "| -t | .1 | Hold out 10% sequences for testing. |\n",
    "| -v | .1 | Hold out 10% sequences for validation. |\n",
    "| -w | 128 | Pool the nucleotide-resolution values to 128 bp bins. |\n",
    "| fasta_file| data/hg19.ml.fa | FASTA file to extract sequences from. |\n",
    "| targets_file | data/heart_wigs.txt | Target samples table with BigWig paths. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contigs divided into\n",
      " Train:  4701 contigs, 2169074921 nt (0.8005)\n",
      " Valid:   572 contigs,  270358978 nt (0.0998)\n",
      " Test:    584 contigs,  270330829 nt (0.0998)\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs11760.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/0.h5 &> data/heart_l131k/seqs_cov/0.err\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12843.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/1.h5 &> data/heart_l131k/seqs_cov/1.err\n",
      "basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12856.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/2.h5 &> data/heart_l131k/seqs_cov/2.err\n",
      "basenji_data_write.py -s 0 -e 256 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-0.tfr &> data/heart_l131k/tfrecords/train-0.err\n",
      "basenji_data_write.py -s 256 -e 512 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-1.tfr &> data/heart_l131k/tfrecords/train-1.err\n",
      "basenji_data_write.py -s 512 -e 585 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-2.tfr &> data/heart_l131k/tfrecords/train-2.err\n",
      "basenji_data_write.py -s 1 -e 257 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/valid-0.tfr &> data/heart_l131k/tfrecords/valid-0.err\n",
      "basenji_data_write.py -s 257 -e 513 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/valid-1.tfr &> data/heart_l131k/tfrecords/valid-1.err\n",
      "basenji_data_write.py -s 513 -e 580 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/valid-2.tfr &> data/heart_l131k/tfrecords/valid-2.err\n",
      "basenji_data_write.py -s 26 -e 282 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/test-0.tfr &> data/heart_l131k/tfrecords/test-0.err\n",
      "basenji_data_write.py -s 282 -e 538 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/test-1.tfr &> data/heart_l131k/tfrecords/test-1.err\n",
      "basenji_data_write.py -s 538 -e 579 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/test-2.tfr &> data/heart_l131k/tfrecords/test-2.err\n"
     ]
    }
   ],
   "source": [
    "! basenji_data.py -d .1 -g data/unmap_macro.bed -l 131072 --local -o data/heart_l131k -p 8 -t .1 -v .1 -w 128 data/hg19.ml.fa data/heart_wigs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, data/heart_l131k contains relevant data for training.\n",
    "\n",
    "*contigs.bed* contains the original large contiguous regions from which training sequences were taken (possibly strided).\n",
    "*sequences.bed*\n",
    "\n",
    "contains the train/valid/test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  33 test\n",
      " 513 train\n",
      "  39 valid\n"
     ]
    }
   ],
   "source": [
    "! cut -f4 data/heart_l131k/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr11\t76073761\t76204833\ttrain\n",
      "chrX\t153384241\t153515313\tvalid\n",
      "chr4\t38070907\t38201979\ttrain\n"
     ]
    }
   ],
   "source": [
    "! head -n3 data/heart_l131k/sequences.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrX\t153384241\t153515313\tvalid\n",
      "chr20\t37530417\t37661489\tvalid\n",
      "chr6\t6561286\t6692358\tvalid\n"
     ]
    }
   ],
   "source": [
    "! grep valid data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\t203940542\t204071614\ttest\n",
      "chrX\t150766960\t150898032\ttest\n",
      "chr15\t70554904\t70685976\ttest\n"
     ]
    }
   ],
   "source": [
    "! grep test data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 davidkelley  staff  14404724 Dec 30 08:44 data/heart_l131k/tfrecords/test-0.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  14427753 Dec 30 08:44 data/heart_l131k/tfrecords/test-1.tfr\n",
      "-rw-r--r--  1 davidkelley  staff   2313992 Dec 30 08:43 data/heart_l131k/tfrecords/test-2.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  14410040 Dec 30 08:44 data/heart_l131k/tfrecords/train-0.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  14419617 Dec 30 08:44 data/heart_l131k/tfrecords/train-1.tfr\n",
      "-rw-r--r--  1 davidkelley  staff   4119448 Dec 30 08:43 data/heart_l131k/tfrecords/train-2.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  14408584 Dec 30 08:44 data/heart_l131k/tfrecords/valid-0.tfr\n",
      "-rw-r--r--  1 davidkelley  staff  14421284 Dec 30 08:44 data/heart_l131k/tfrecords/valid-1.tfr\n",
      "-rw-r--r--  1 davidkelley  staff   3780093 Dec 30 08:43 data/heart_l131k/tfrecords/valid-2.tfr\n"
     ]
    }
   ],
   "source": [
    "! ls -l data/heart_l131k/tfrecords/*.tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
