{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../basenji/')\n",
    "\n",
    "from basenji import params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/n/local/basenji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': 'exp_linear', 'batch_buffer': 4096, 'adam_beta1': 0.97, 'cnn_dilation': [1, 1, 1, 1, 1, 1, 2, 4, 8, 16, 32, 64, 1], 'adam_beta2': 0.98, 'cnn_pool': [1, 2, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1], 'loss': 'poisson', 'batch_size': 2, 'num_targets': 4229, 'cnn_dense': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0], 'seq_length': 131072, 'cnn_filters': [312, 368, 435, 514, 607, 717, 108, 108, 108, 108, 108, 108, 1365], 'cnn_dropout': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05], 'cnn_filter_sizes': [22, 1, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 1], 'learning_rate': 0.002063}\n"
     ]
    }
   ],
   "source": [
    "params_file = os.path.join(root_dir,'manuscript/params.txt')\n",
    "job = params.read_job_params(params_file, require=['seq_length','num_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(root_dir,'manuscript/model_numpy.pkl'),'rb') as f:\n",
    "    check_weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasenjiConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, \n",
    "                 dense_conv=False, padding_mode='zeros', use_batchnorm=True,\n",
    "                 pooling_type='max', pooling_kernel=1, pooling_stride=1,\n",
    "                 activation_type='relu', dropout_prob=0.5):\n",
    "        super(BasenjiConvBlock, self).__init__()\n",
    "        # Set 1D convolution\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                stride, padding, dilation, groups, bias,\n",
    "                                padding_mode)\n",
    "        # Set Batch Norm layer\n",
    "        if use_batchnorm:\n",
    "            self.batch_normalization = nn.BatchNorm1d(out_channels, eps=1e-05, \n",
    "                                                      momentum=0.1, affine=True, \n",
    "                                                      track_running_stats=True)\n",
    "        else:\n",
    "            self.batch_normalization = Passthrough()\n",
    "        # Add pooling\n",
    "        if pooling_type == 'max':\n",
    "            self.pooling = nn.MaxPool1d(pooling_kernel, pooling_stride)\n",
    "        else:\n",
    "            print('Unrecognized pooling type \"{}\"'.format(pooling_type), file=sys.stderr)\n",
    "            exit(1)\n",
    "        # Add Nonlinearity\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'exp_linear':\n",
    "            self.activation = ExpLinear()\n",
    "        elif activation_type == 'none':\n",
    "            self.activation = Passthrough()\n",
    "        else:\n",
    "            print('Unrecognized activation type \"{}\"'.format(activation_type), file=sys.stderr)\n",
    "        # Add dropout\n",
    "        if dropout_prob > 0.0:\n",
    "            self.dropout = Dropout1d(p=dropout_prob)\n",
    "        else:\n",
    "            self.dropout = Passthrough()\n",
    "        # Decide if dense conv\n",
    "        self.dense_conv = dense_conv\n",
    "    def forward(self, input):\n",
    "        hook = self.conv1d( input )\n",
    "        hook = self.batch_normalization( hook )\n",
    "        hook = self.pooling( hook )\n",
    "        hook = self.activation( hook )\n",
    "        hook = self.dropout( hook )\n",
    "        if self.dense_conv:\n",
    "            hook = torch.cat([input,hook],dim=1)\n",
    "        return hook\n",
    "    \n",
    "class Dropout1d(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout1d, self).__init__()\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "    def forward(self, input):\n",
    "        return self.dropout( input.unsqueeze(-1) ).squeeze(3)\n",
    "\n",
    "class ExpLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExpLinear, self).__init__()\n",
    "    def forward(self, input):\n",
    "        return input.clamp(min=0.) + \\\n",
    "               input.clamp(min=-50., max=0.).exp()\n",
    "\n",
    "class Passthrough(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Passthrough, self).__init__()\n",
    "    def forward(self, input):\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqNN(nn.Module):\n",
    "    def __init__(self, job):\n",
    "        super(SeqNN, self).__init__()\n",
    "        network = []\n",
    "        density = 0\n",
    "        input_feat = 4\n",
    "        for i in range(len(job['cnn_filter_sizes'])):\n",
    "            curr_feat = job['cnn_filters'][i]\n",
    "            dilation  = job['cnn_dilation'][i]\n",
    "            kern_size = job['cnn_filter_sizes'][i]\n",
    "            drop_prob = job['cnn_dropout'][i]\n",
    "            pooling   = job['cnn_pool'][i]\n",
    "            is_dense  = job['cnn_dense'][i] == 1\n",
    "            if is_dense:\n",
    "                density += job['cnn_filters'][i-1]\n",
    "            network.append(\n",
    "                ('cnn{}'.format(i), BasenjiConvBlock(input_feat, curr_feat, kern_size,\n",
    "                                                      stride=1, padding=0, dilation=dilation, \n",
    "                                                      groups=1, bias=False, dense_conv=is_dense, \n",
    "                                                      pooling_kernel=pooling, pooling_stride=pooling, \n",
    "                                                      activation_type='relu', dropout_prob=drop_prob\n",
    "                                                     ) )\n",
    "            )\n",
    "            input_feat = curr_feat + density\n",
    "\n",
    "\n",
    "        network.append(\n",
    "            ('final', BasenjiConvBlock(curr_feat, job['num_targets'], 1, stride=1, \n",
    "                                       padding=0, dilation=1, groups=1, bias=True, dense_conv=False,\n",
    "                                       use_batchnorm=False, pooling_kernel=1, pooling_stride=1, \n",
    "                                       activation_type='exp_linear', dropout_prob=0.0\n",
    "                                      ))\n",
    "        )\n",
    "        self.network = nn.Sequential( OrderedDict(network) )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.network(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basenji = SeqNN(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (cnn0): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(4, 312, kernel_size=(22,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn1): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(312, 368, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn2): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(368, 435, kernel_size=(6,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(435, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn3): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(435, 514, kernel_size=(6,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(514, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn4): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(514, 607, kernel_size=(6,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(607, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn5): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(607, 717, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(717, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (cnn6): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(717, 108, kernel_size=(3,), stride=(1,), dilation=(2,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn7): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(825, 108, kernel_size=(3,), stride=(1,), dilation=(4,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn8): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(933, 108, kernel_size=(3,), stride=(1,), dilation=(8,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn9): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(1041, 108, kernel_size=(3,), stride=(1,), dilation=(16,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn10): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(1149, 108, kernel_size=(3,), stride=(1,), dilation=(32,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn11): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(1257, 108, kernel_size=(3,), stride=(1,), dilation=(64,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (cnn12): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(1365, 1365, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (batch_normalization): BatchNorm1d(1365, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout1d(\n",
       "      (dropout): Dropout2d(p=0.05)\n",
       "    )\n",
       "  )\n",
       "  (final): BasenjiConvBlock(\n",
       "    (conv1d): Conv1d(1365, 4229, kernel_size=(1,), stride=(1,))\n",
       "    (batch_normalization): Passthrough()\n",
       "    (pooling): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (activation): ExpLinear()\n",
       "    (dropout): Passthrough()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Basenji.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(13):\n",
    "    new_weights = torch.Tensor(check_weights['cnn{}/conv1d/kernel:0'.format(i)]).permute(2,1,0)\n",
    "    new_gamma   = torch.Tensor(check_weights['cnn{}/batch_normalization/gamma:0'.format(i)])\n",
    "    new_beta    = torch.Tensor(check_weights['cnn{}/batch_normalization/beta:0'.format(i)])\n",
    "    new_mean    = torch.Tensor(check_weights['cnn{}/batch_normalization/moving_mean:0'.format(i)])\n",
    "    new_var     = torch.Tensor(check_weights['cnn{}/batch_normalization/moving_variance:0'.format(i)])\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).conv1d.weight.data = new_weights\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).batch_normalization.weight.data = new_gamma\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).batch_normalization.bias.data = new_beta\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).batch_normalization.running_mean.data = new_mean\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).batch_normalization.running_var.data = new_var\n",
    "    getattr( Basenji.network, 'cnn{}'.format(i) ).batch_normalization.num_batches_tracked.data = torch.tensor(10000, dtype=torch.long)\n",
    "\n",
    "new_weights = torch.Tensor(check_weights['final/dense/kernel:0']).permute(1,0).unsqueeze(2)\n",
    "new_bias    = torch.Tensor(check_weights['final/dense/bias:0'])\n",
    "getattr( Basenji.network, 'final' ).conv1d.weight.data = new_weights\n",
    "getattr( Basenji.network, 'final' ).conv1d.bias.data = new_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basenji]",
   "language": "python",
   "name": "conda-env-basenji-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
